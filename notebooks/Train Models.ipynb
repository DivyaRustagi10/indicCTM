{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Te7oLPMkvQR_"
   },
   "source": [
    "#  ZeroShot Topic Modeling for Indic Languages (IndicCTM)\n",
    "*Previously Zero-shot Cross-Lingual Topic Modeling For Same Script Languages*\n",
    "> Do conxtextualized TM tackle zero-shot cross-lingual topic modeling better on same script languages?\n",
    "\n",
    "We use 4000 documents as training and consider randomly sampled 800 documents as the test set. We collect the 800 respective instances in Hindi (hi), Assamese (as), Gujarati (gu), Kannada (kn), Malayalam (ml), Marathi (mr), Oriya (or), Punjabi (pa), Tamil (ta), Telugu (te) and English (en).\n",
    "\n",
    "First, we use IndicBERT to generate multilingual embeddings as the input of the model. Then we evaluate multilingual topic predictions on the multilingual abstracts in test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RntUGTeMeRu7"
   },
   "source": [
    "## Hardware Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5uJXEgaedqA"
   },
   "source": [
    "1. Setup new env with py38 (anaconda)\n",
    "2. Setup PyTorch + CUDA + GPU\n",
    "https://medium.com/@leennewlife/how-to-setup-pytorch-with-cuda-in-windows-11-635dfa56724b\n",
    "\n",
    "3. Setup Colab (Run in cwd\\drustagi)\n",
    "https://krishnacheedella.medium.com/connect-to-a-local-runtime-from-google-colab-to-your-local-jupyter-notebook-on-http-over-websocket-f7b4bd4eb585\n",
    "\n",
    "4. Run Hardware Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "mfOMT8Lyqu3b",
    "outputId": "1f372802-0ba9-4c12-be05-e668c3a7bfe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.0a0+fe05266\n",
      "**********\n",
      "_CUDA version: \n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Tue_Feb__7_19:32:13_PST_2023\n",
      "Cuda compilation tools, release 12.1, V12.1.66\n",
      "Build cuda_12.1.r12.1/compiler.32415258_0\n",
      "**********\n",
      "CUDNN version: 8900\n",
      "Available GPU devices: 1\n",
      "Device Name: NVIDIA GeForce GTX 1660\n"
     ]
    }
   ],
   "source": [
    "4# CUDA Check (Local Runtime)\n",
    "import torch\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print('*'*10)\n",
    "print(f'_CUDA version: ')\n",
    "!nvcc --version\n",
    "print('*'*10)\n",
    "print(f'CUDNN version: {torch.backends.cudnn.version()}')\n",
    "print(f'Available GPU devices: {torch.cuda.device_count()}')\n",
    "print(f'Device Name: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCeeGc0ykG25"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (23.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyter in /usr/local/lib/python3.8/dist-packages (1.0.0)\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.8/dist-packages (from jupyter) (6.4.10)\n",
      "Requirement already satisfied: qtconsole in /usr/local/lib/python3.8/dist-packages (from jupyter) (5.4.3)\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.8/dist-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.8/dist-packages (from jupyter) (7.3.1)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.8/dist-packages (from jupyter) (6.22.0)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.8/dist-packages (from jupyter) (8.0.6)\n",
      "Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter) (0.1.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter) (8.12.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter) (8.2.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter) (5.3.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter) (1.5.6)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter) (23.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter) (5.9.4)\n",
      "Requirement already satisfied: pyzmq>=20 in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter) (25.0.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter) (6.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in /usr/local/lib/python3.8/dist-packages (from ipywidgets->jupyter) (4.0.7)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /usr/local/lib/python3.8/dist-packages (from ipywidgets->jupyter) (3.0.7)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in /usr/local/lib/python3.8/dist-packages (from jupyter-console->jupyter) (3.0.38)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from jupyter-console->jupyter) (2.15.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter) (4.12.2)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter) (6.0.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter) (6.3.0)\n",
      "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter) (3.1.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter) (0.2.2)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter) (2.1.2)\n",
      "Requirement already satisfied: mistune<3,>=2.0.3 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter) (2.0.5)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter) (0.7.3)\n",
      "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter) (5.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter) (1.2.1)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter) (21.3.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter) (0.2.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter) (0.17.1)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter) (0.16.0)\n",
      "Requirement already satisfied: qtpy>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from qtconsole->jupyter) (2.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=3.6->nbconvert->jupyter) (3.15.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.18.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.7.5)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.8/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.5.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.8/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (3.2.0)\n",
      "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat>=5.1->nbconvert->jupyter) (2.16.3)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat>=5.1->nbconvert->jupyter) (4.17.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter) (0.2.6)\n",
      "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.8/dist-packages (from terminado>=0.8.3->notebook->jupyter) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.8/dist-packages (from argon2-cffi->notebook->jupyter) (21.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.8/dist-packages (from beautifulsoup4->nbconvert->jupyter) (2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert->jupyter) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter) (22.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter) (5.12.0)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter) (1.3.10)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter) (0.19.3)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (1.15.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (0.2.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.21)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.8/dist-packages (25.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tensorflow==2.12.* in /usr/local/lib/python3.8/dist-packages (2.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (23.5.9)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (1.53.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (3.8.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (0.4.10)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (2.12.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (16.0.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (1.22.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (65.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (2.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (4.5.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.12.*) (0.32.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.*) (0.40.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.15->tensorflow==2.12.*) (0.1.0)\n",
      "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.15->tensorflow==2.12.*) (1.10.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.*) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.*) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.*) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.*) (2.28.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.*) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.*) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.*) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.*) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.*) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.*) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow==2.12.*) (6.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.*) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.*) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.*) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.*) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.*) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow==2.12.*) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.*) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.*) (3.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Always use the most updated version of pip and jupyter notebook\n",
    "!pip install --upgrade pip\n",
    "!pip install --upgrade jupyter\n",
    "!pip install --upgrade pyzmq\n",
    "!pip install tensorflow==2.12.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_rvOPdVkJ62"
   },
   "source": [
    "### Install Libraries and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4XdDiDPfU3c"
   },
   "source": [
    "**Use Python Version 3.8.16**\n",
    "> *3.10 does not support collections, so cannot use.*\n",
    ">>Affected packages - inltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting contextualized_topic_models\n",
      "  Downloading contextualized_topic_models-2.5.0-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.8/dist-packages (from contextualized_topic_models) (1.22.2)\n",
      "Requirement already satisfied: torchvision>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from contextualized_topic_models) (0.15.0a0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from contextualized_topic_models) (2.1.0a0+fe05266)\n",
      "Collecting gensim==4.2.0 (from contextualized_topic_models)\n",
      "  Downloading gensim-4.2.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentence-transformers>=2.1.1 (from contextualized_topic_models)\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting wordcloud>=1.8.1 (from contextualized_topic_models)\n",
      "  Downloading wordcloud-1.9.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m461.4/461.4 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.1.3 in /usr/local/lib/python3.8/dist-packages (from contextualized_topic_models) (3.7.1)\n",
      "Requirement already satisfied: tqdm>=4.56.0 in /usr/local/lib/python3.8/dist-packages (from contextualized_topic_models) (4.65.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from contextualized_topic_models) (1.10.1)\n",
      "Collecting ipywidgets==7.5.1 (from contextualized_topic_models)\n",
      "  Downloading ipywidgets-7.5.1-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 kB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ipython==8.10.0 (from contextualized_topic_models)\n",
      "  Downloading ipython-8.10.0-py3-none-any.whl (784 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m784.3/784.3 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim==4.2.0->contextualized_topic_models) (6.3.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython==8.10.0->contextualized_topic_models) (0.2.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython==8.10.0->contextualized_topic_models) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython==8.10.0->contextualized_topic_models) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.8/dist-packages (from ipython==8.10.0->contextualized_topic_models) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython==8.10.0->contextualized_topic_models) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.30 in /usr/local/lib/python3.8/dist-packages (from ipython==8.10.0->contextualized_topic_models) (3.0.38)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from ipython==8.10.0->contextualized_topic_models) (2.15.0)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.8/dist-packages (from ipython==8.10.0->contextualized_topic_models) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in /usr/local/lib/python3.8/dist-packages (from ipython==8.10.0->contextualized_topic_models) (5.9.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.8/dist-packages (from ipython==8.10.0->contextualized_topic_models) (4.8.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets==7.5.1->contextualized_topic_models) (6.22.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets==7.5.1->contextualized_topic_models) (5.8.0)\n",
      "Collecting widgetsnbextension~=3.5.0 (from ipywidgets==7.5.1->contextualized_topic_models)\n",
      "  Downloading widgetsnbextension-3.5.2-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.1.3->contextualized_topic_models) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.1.3->contextualized_topic_models) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.1.3->contextualized_topic_models) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.1.3->contextualized_topic_models) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.1.3->contextualized_topic_models) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.1.3->contextualized_topic_models) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.1.3->contextualized_topic_models) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.1.3->contextualized_topic_models) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.1.3->contextualized_topic_models) (5.12.0)\n",
      "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers>=2.1.1->contextualized_topic_models)\n",
      "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=2.1.1->contextualized_topic_models) (1.2.0)\n",
      "Collecting nltk (from sentence-transformers>=2.1.1->contextualized_topic_models)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece (from sentence-transformers>=2.1.1->contextualized_topic_models)\n",
      "  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers>=2.1.1->contextualized_topic_models)\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->contextualized_topic_models) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->contextualized_topic_models) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->contextualized_topic_models) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->contextualized_topic_models) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->contextualized_topic_models) (3.1.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.7.0->contextualized_topic_models) (2.28.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.1.1->contextualized_topic_models) (2023.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.1.1->contextualized_topic_models) (6.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3.1.3->contextualized_topic_models) (3.15.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized_topic_models) (0.1.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized_topic_models) (1.6.7)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized_topic_models) (8.2.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized_topic_models) (5.3.0)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized_topic_models) (1.5.6)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized_topic_models) (5.9.4)\n",
      "Requirement already satisfied: pyzmq>=20 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized_topic_models) (25.0.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized_topic_models) (6.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython==8.10.0->contextualized_topic_models) (0.8.3)\n",
      "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat>=4.2.0->ipywidgets==7.5.1->contextualized_topic_models) (2.16.3)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat>=4.2.0->ipywidgets==7.5.1->contextualized_topic_models) (4.17.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3->ipython==8.10.0->contextualized_topic_models) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython==8.10.0->contextualized_topic_models) (0.2.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.3->contextualized_topic_models) (1.16.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.1.1->contextualized_topic_models) (2023.3.23)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.1.1->contextualized_topic_models)\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.8/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models) (6.4.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.6.0->contextualized_topic_models) (2.1.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers>=2.1.1->contextualized_topic_models) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers>=2.1.1->contextualized_topic_models) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.7.0->contextualized_topic_models) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.7.0->contextualized_topic_models) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.7.0->contextualized_topic_models) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.7.0->contextualized_topic_models) (2022.12.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers>=2.1.1->contextualized_topic_models) (3.1.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython==8.10.0->contextualized_topic_models) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython==8.10.0->contextualized_topic_models) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython==8.10.0->contextualized_topic_models) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.6.0->contextualized_topic_models) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets==7.5.1->contextualized_topic_models) (22.2.0)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets==7.5.1->contextualized_topic_models) (1.3.10)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets==7.5.1->contextualized_topic_models) (0.19.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in /usr/local/lib/python3.8/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized_topic_models) (6.3.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized_topic_models) (3.2.0)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models) (21.3.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models) (0.2.0)\n",
      "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models) (7.3.1)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models) (0.17.1)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models) (0.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models) (4.12.2)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models) (6.0.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.8/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models) (0.2.2)\n",
      "Requirement already satisfied: mistune<3,>=2.0.3 in /usr/local/lib/python3.8/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models) (2.0.5)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models) (0.7.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.8/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models) (1.2.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.8/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models) (1.15.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.8/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models) (2.4)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models) (0.5.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models) (2.21)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=58e98c3c52f3b101380e8b3f5205660a63c7514985c9c3b9ce1393c4d17321c9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5ghdlppz/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: tokenizers, sentencepiece, nltk, huggingface-hub, gensim, wordcloud, transformers, ipython, sentence-transformers, widgetsnbextension, ipywidgets, contextualized_topic_models\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 8.12.0\n",
      "    Uninstalling ipython-8.12.0:\n",
      "      Successfully uninstalled ipython-8.12.0\n",
      "  Attempting uninstall: widgetsnbextension\n",
      "    Found existing installation: widgetsnbextension 4.0.7\n",
      "    Uninstalling widgetsnbextension-4.0.7:\n",
      "      Successfully uninstalled widgetsnbextension-4.0.7\n",
      "  Attempting uninstall: ipywidgets\n",
      "    Found existing installation: ipywidgets 8.0.6\n",
      "    Uninstalling ipywidgets-8.0.6:\n",
      "      Successfully uninstalled ipywidgets-8.0.6\n",
      "Successfully installed contextualized_topic_models-2.5.0 gensim-4.2.0 huggingface-hub-0.14.1 ipython-8.10.0 ipywidgets-7.5.1 nltk-3.8.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.29.1 widgetsnbextension-3.5.2 wordcloud-1.9.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting pyldavis\n",
      "  Downloading pyLDAvis-3.4.0-py3-none-any.whl (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.8/dist-packages (from pyldavis) (1.22.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from pyldavis) (1.10.1)\n",
      "Requirement already satisfied: pandas>=1.3.4 in /usr/local/lib/python3.8/dist-packages (from pyldavis) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from pyldavis) (1.2.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from pyldavis) (3.1.2)\n",
      "Collecting numexpr (from pyldavis)\n",
      "  Downloading numexpr-2.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.7/381.7 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting funcy (from pyldavis)\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from pyldavis) (1.2.0)\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (from pyldavis) (4.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from pyldavis) (65.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.4->pyldavis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.4->pyldavis) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->pyldavis) (3.1.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim->pyldavis) (6.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->pyldavis) (2.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas>=1.3.4->pyldavis) (1.16.0)\n",
      "Installing collected packages: funcy, numexpr, pyldavis\n",
      "Successfully installed funcy-2.0 numexpr-2.8.4 pyldavis-3.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9676 sha256=b48bf0436f0439401b20f1a91a6d0fc183fc6027f71e0d2fa895c60969b29ddc\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-o9is7xji/wheels/bd/a8/c3/3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting head\n",
      "  Downloading Head-1.0.0.zip (837 bytes)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: head\n",
      "  Building wheel for head (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for head: filename=Head-1.0.0-py3-none-any.whl size=1254 sha256=aa3db3c77571dc1fb5096e7eb060379f56c1b0b21ae59368504ca0f08348edb2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-89sjrov5/wheels/ac/b3/fc/61502de0941ef1c342581c9824780403c82ab143a5169ebe47\n",
      "Successfully built head\n",
      "Installing collected packages: head\n",
      "Successfully installed head-1.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mSun May 14 22:44:28 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.50                 Driver Version: 531.79       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1660         On | 00000000:2B:00.0  On |                  N/A |\n",
      "|  0%   51C    P8               11W / 130W|    925MiB /  6144MiB |     13%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A        20      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A        23      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A        26      G   /Xwayland                                 N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting indic-nlp-library==0.81\n",
      "  Downloading indic_nlp_library-0.81-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m710.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sphinx-argparse (from indic-nlp-library==0.81)\n",
      "  Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\n",
      "Collecting sphinx-rtd-theme (from indic-nlp-library==0.81)\n",
      "  Downloading sphinx_rtd_theme-1.2.0-py2.py3-none-any.whl (2.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting morfessor (from indic-nlp-library==0.81)\n",
      "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from indic-nlp-library==0.81) (1.5.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from indic-nlp-library==0.81) (1.22.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->indic-nlp-library==0.81) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->indic-nlp-library==0.81) (2023.3)\n",
      "Collecting sphinx>=1.2.0 (from sphinx-argparse->indic-nlp-library==0.81)\n",
      "  Downloading sphinx-7.0.1-py3-none-any.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading sphinx-6.2.1-py3-none-any.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting docutils<0.19 (from sphinx-rtd-theme->indic-nlp-library==0.81)\n",
      "  Downloading docutils-0.18.1-py2.py3-none-any.whl (570 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m570.0/570.0 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sphinxcontrib-jquery!=3.0.0,>=2.0.0 (from sphinx-rtd-theme->indic-nlp-library==0.81)\n",
      "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->indic-nlp-library==0.81) (1.16.0)\n",
      "Collecting sphinxcontrib-applehelp (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81)\n",
      "  Downloading sphinxcontrib_applehelp-1.0.4-py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.6/120.6 kB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sphinxcontrib-devhelp (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81)\n",
      "  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sphinxcontrib-jsmath (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81)\n",
      "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.0 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81)\n",
      "  Downloading sphinxcontrib_htmlhelp-2.0.1-py3-none-any.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.8/99.8 kB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sphinxcontrib-serializinghtml>=1.1.5 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81)\n",
      "  Downloading sphinxcontrib_serializinghtml-1.1.5-py2.py3-none-any.whl (94 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sphinxcontrib-qthelp (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81)\n",
      "  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.8/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81) (3.1.2)\n",
      "Requirement already satisfied: Pygments>=2.13 in /usr/local/lib/python3.8/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81) (2.15.0)\n",
      "Collecting snowballstemmer>=2.0 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81)\n",
      "  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 kB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting babel>=2.9 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81)\n",
      "  Downloading Babel-2.12.1-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting alabaster<0.8,>=0.7 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81)\n",
      "  Downloading alabaster-0.7.13-py3-none-any.whl (13 kB)\n",
      "Collecting imagesize>=1.3 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81)\n",
      "  Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.8/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81) (2.28.2)\n",
      "Requirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.8/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81) (23.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8 in /usr/local/lib/python3.8/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81) (6.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81) (3.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from Jinja2>=3.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library==0.81) (2022.12.7)\n",
      "Installing collected packages: snowballstemmer, morfessor, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, imagesize, docutils, babel, alabaster, sphinx, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
      "Successfully installed alabaster-0.7.13 babel-2.12.1 docutils-0.18.1 imagesize-1.4.1 indic-nlp-library-0.81 morfessor-2.0.6 snowballstemmer-2.2.0 sphinx-6.2.1 sphinx-argparse-0.4.0 sphinx-rtd-theme-1.2.0 sphinxcontrib-applehelp-1.0.4 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.1 sphinxcontrib-jquery-4.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 sphinxcontrib-serializinghtml-1.1.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting stopwordsiso\n",
      "  Downloading stopwordsiso-0.6.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: stopwordsiso\n",
      "Successfully installed stopwordsiso-0.6.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting inltk\n",
      "  Downloading inltk-0.9-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: aiohttp>=3.5.4 in /usr/local/lib/python3.8/dist-packages (from inltk) (3.8.4)\n",
      "Requirement already satisfied: async-timeout>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from inltk) (4.0.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from inltk) (9.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from inltk) (4.12.2)\n",
      "Collecting bottleneck (from inltk)\n",
      "  Downloading Bottleneck-1.3.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.2/355.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fastprogress>=0.1.19 (from inltk)\n",
      "  Downloading fastprogress-1.0.3-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from inltk) (3.7.1)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.8/dist-packages (from inltk) (2.8.4)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.8/dist-packages (from inltk) (1.22.2)\n",
      "Collecting nvidia-ml-py3 (from inltk)\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from inltk) (23.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from inltk) (1.5.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from inltk) (6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from inltk) (2.28.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from inltk) (1.10.1)\n",
      "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.8/dist-packages (from inltk) (3.5.2)\n",
      "Collecting typing (from inltk)\n",
      "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fastai==1.0.57 (from inltk)\n",
      "  Downloading fastai-1.0.57-py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.3/233.3 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from inltk) (0.1.99)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from fastai==1.0.57->inltk) (2.1.0a0+fe05266)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from fastai==1.0.57->inltk) (0.15.0a0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp>=3.5.4->inltk) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp>=3.5.4->inltk) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp>=3.5.4->inltk) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp>=3.5.4->inltk) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp>=3.5.4->inltk) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp>=3.5.4->inltk) (1.3.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->inltk) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->inltk) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->inltk) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->inltk) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->inltk) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->inltk) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->inltk) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->inltk) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->inltk) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->inltk) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->inltk) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->inltk) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->inltk) (4.65.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->inltk) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->inltk) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->inltk) (65.5.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.18->inltk) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->inltk) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->inltk) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->inltk) (2022.12.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.8/dist-packages (from beautifulsoup4->inltk) (2.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->inltk) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->inltk) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->inltk) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->inltk) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->inltk) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->inltk) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->inltk) (5.12.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->inltk) (2023.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=3.2.0->matplotlib->inltk) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy>=2.0.18->inltk) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib->inltk) (1.16.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.0.18->inltk) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.0.18->inltk) (0.0.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch>=1.0.0->fastai==1.0.57->inltk) (3.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.0.0->fastai==1.0.57->inltk) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.0.0->fastai==1.0.57->inltk) (2.6.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy>=2.0.18->inltk) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy>=2.0.18->inltk) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.0.0->fastai==1.0.57->inltk) (1.3.0)\n",
      "Building wheels for collected packages: nvidia-ml-py3, typing\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19188 sha256=19720ff43310abf78bb34321097a0fa9eab0040f78ada268c60769f6231fe943\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-p40vyhlq/wheels/b9/b1/68/cb4feab29709d4155310d29a421389665dcab9eb3b679b527b\n",
      "  Building wheel for typing (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26321 sha256=8b3251024a9f48249a78494ac519fd0a878054fb25a699f3250854d100ccfea0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-p40vyhlq/wheels/5e/5d/01/3083e091b57809dad979ea543def62d9d878950e3e74f0c930\n",
      "Successfully built nvidia-ml-py3 typing\n",
      "Installing collected packages: nvidia-ml-py3, typing, fastprogress, bottleneck, fastai, inltk\n",
      "Successfully installed bottleneck-1.3.7 fastai-1.0.57 fastprogress-1.0.3 inltk-0.9 nvidia-ml-py3-7.352.0 typing-3.7.4.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (2023.3.23)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting urduhack\n",
      "  Downloading urduhack-1.1.1-py3-none-any.whl (105 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.5/105.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tf2crf (from urduhack)\n",
      "  Downloading tf2crf-0.1.33-py2.py3-none-any.whl (7.3 kB)\n",
      "Collecting tensorflow-datasets~=3.1 (from urduhack)\n",
      "  Downloading tensorflow_datasets-3.2.1-py3-none-any.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting Click~=7.1 (from urduhack)\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from urduhack) (2023.3.23)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.4.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets~=3.1->urduhack) (22.2.0)\n",
      "Collecting dill (from tensorflow-datasets~=3.1->urduhack)\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting future (from tensorflow-datasets~=3.1->urduhack)\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.22.2)\n",
      "Collecting promise (from tensorflow-datasets~=3.1->urduhack)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets~=3.1->urduhack) (3.20.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets~=3.1->urduhack) (2.28.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.16.0)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets~=3.1->urduhack)\n",
      "  Downloading tensorflow_metadata-1.13.1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets~=3.1->urduhack) (2.3.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets~=3.1->urduhack) (4.65.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.14.1)\n",
      "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from tf2crf->urduhack) (2.12.0)\n",
      "Collecting tensorflow-addons>=0.8.2 (from tf2crf->urduhack)\n",
      "  Downloading tensorflow_addons-0.20.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (2022.12.7)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (23.5.9)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.53.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (3.8.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.4.10)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.12.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (16.0.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (23.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (65.5.1)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (4.5.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.32.0)\n",
      "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons>=0.8.2->tf2crf->urduhack)\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0 (from tensorflow-metadata->tensorflow-datasets~=3.1->urduhack)\n",
      "  Downloading googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow>=2.1.0->tf2crf->urduhack) (0.40.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.15->tensorflow>=2.1.0->tf2crf->urduhack) (0.1.0)\n",
      "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.15->tensorflow>=2.1.0->tf2crf->urduhack) (1.10.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.1.0->tf2crf->urduhack) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.1.0->tf2crf->urduhack) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.1.0->tf2crf->urduhack) (3.4.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.1.0->tf2crf->urduhack) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.1.0->tf2crf->urduhack) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.1.0->tf2crf->urduhack) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.1.0->tf2crf->urduhack) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.1.0->tf2crf->urduhack) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.1.0->tf2crf->urduhack) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow>=2.1.0->tf2crf->urduhack) (6.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.1.0->tf2crf->urduhack) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow>=2.1.0->tf2crf->urduhack) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.1.0->tf2crf->urduhack) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.1.0->tf2crf->urduhack) (3.2.2)\n",
      "Building wheels for collected packages: future, promise\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492035 sha256=02dabbb878f7fa08be6deddf9dcd57d9de1ddc5eb8109e707053f9f6a9055798\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-qye6yitw/wheels/a0/0b/ee/e6994fadb42c1354dcccb139b0bf2795271bddfe6253ccdf11\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21501 sha256=0fa6b3edaad749d0cf6bbbe90cde0b6577a2190a8c348e82406623837d537666\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-qye6yitw/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "Successfully built future promise\n",
      "Installing collected packages: typeguard, promise, googleapis-common-protos, future, dill, Click, tensorflow-metadata, tensorflow-addons, tensorflow-datasets, tf2crf, urduhack\n",
      "  Attempting uninstall: Click\n",
      "    Found existing installation: click 8.1.3\n",
      "    Uninstalling click-8.1.3:\n",
      "      Successfully uninstalled click-8.1.3\n",
      "Successfully installed Click-7.1.2 dill-0.3.6 future-0.18.3 googleapis-common-protos-1.59.0 promise-2.3 tensorflow-addons-0.20.0 tensorflow-datasets-3.2.1 tensorflow-metadata-1.13.1 tf2crf-0.1.33 typeguard-2.13.3 urduhack-1.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install the contextualized topic model library\n",
    "!pip install -U contextualized_topic_models\n",
    "\n",
    "!pip install pyldavis\n",
    "!pip install wget\n",
    "!pip install head\n",
    "!nvidia-smi\n",
    "\n",
    "# Setup Hindi for analysis\n",
    "!pip install indic-nlp-library==0.81\n",
    "!pip install stopwordsiso\n",
    "!pip install inltk\n",
    "!pip install regex\n",
    "!pip install urduhack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yTkSBfbBq25Z"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from contextualized_topic_models.models.ctm import ZeroShotTM\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "from contextualized_topic_models.utils.preprocessing import  WhiteSpacePreprocessingStopwords\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zaM6lH0GjDsR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 22:45:14.966603: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-14 22:45:16.248157: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-14 22:45:16.248799: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Python3.10 issue doesn't work\n",
    "# Either downgrade or use from collections.abc import Iterable\n",
    "from inltk.inltk import setup\n",
    "try:\n",
    "  setup('hi')\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVPQoVENZ-mr"
   },
   "source": [
    "## Load Data\n",
    "\n",
    "**PMIndia Corpus**: \n",
    "Parallel corpus from the website of the Indian Prime Minister (www.pmindia.gov.in). \n",
    "\n",
    "We combine each speech document into one, for every language. Datasets are downloaded from [Statistical Machine Translation](https://data.statmt.org/pmindia/v1/monolingual/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_HLUyqCLifq1"
   },
   "outputs": [],
   "source": [
    "with open('train_speeches.pkl', 'rb') as train_s:\n",
    "    train_speeches = pickle.load(train_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vkfxcP4mk-W"
   },
   "source": [
    "*Split Parallel Corpus Into Test and Train*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PMPHUR_D4f9e",
    "outputId": "ba6b351f-4ce0-484b-9669-e6e3264a1e36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  नागरिकों से स्वच्छाग्रही बनने और स्वच्छ भारत ब...\n",
      "1  श्री सोमनाथ न्यास के न्यासियों की 116वीं बैठक ...\n",
      "2  प्रधानमंत्री श्री नरेन्द्र मोदी की अध्यक्षता म...\n",
      "3  प्रधानमंत्री श्री नरेन्द्र मोदी ने आज ही के दि...\n",
      "4  · खूंटी की जिला अदालत में छत पर लगने वाले सौर ...\n",
      "                                                   0\n",
      "0  PM Calls upon citizens to become Swachhagrahis...\n",
      "1  The first 10 months of Prime Minister Narendra...\n",
      "2  BRICS in Africa: Collaboration for Inclusive G...\n",
      "3  The 116th meeting of the trustees of Shri Somn...\n",
      "4  Deendayal Upadhyaya Gram Jyoti Yojana\\n The Un...\n",
      "\n",
      "Hindi count:0    4003\n",
      "dtype: int64\n",
      "\n",
      "English count:0    4489\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import re\n",
    "\n",
    "# Selecting Train speeches\n",
    "hindi_unprep = pd.DataFrame(list for list in train_speeches['hi'])\n",
    "english_unprep = pd.DataFrame(list for list in train_speeches['en'])\n",
    "\n",
    "# View each row as a speech\n",
    "print(hindi_unprep[:5])\n",
    "print(english_unprep[:5])\n",
    "\n",
    "# n = 4003\n",
    "print(\"\\nHindi count:\" + str(hindi_unprep.count()) + \"\\n\")\n",
    "# n = 4489\n",
    "print(\"English count:\" + str(english_unprep.count()) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEcvJVNDnF02"
   },
   "source": [
    "# Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7NsHKg2WXZS"
   },
   "source": [
    "## **Hindi-Based ZeroshotTM**\n",
    "\n",
    "*Preprocessing*\n",
    "\n",
    "Why do we use the preprocessed text here? We need text without punctuation to build the bag of word. Also, we might want only to have the most frequent words inside the BoW. Too many words might not help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "erO2thoszs7P"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from contextualized_topic_models.models.ctm import ZeroShotTM\n",
    "import contextualized_topic_models.utils.data_preparation\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "from contextualized_topic_models.utils.preprocessing import WhiteSpacePreprocessing\n",
    "import nltk\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8NOdgoBJMGd"
   },
   "source": [
    "**Modify WhiteSpacePreprocessingStopwords to support Hindi text**\n",
    "\n",
    "Above class method is taken from author's original code and modified to support preprocessing of Hindi documents. Additional helper methods have been added to support the updated preprocessing() method.\n",
    "\n",
    "FIXED: Returns expected preprocessed hindi texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nbFHdpLD-f4G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model. This might take time, depending on your internet connection. Please be patient.\n",
      "We'll only do this for the first time.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "from nltk.corpus import stopwords as stop_words\n",
    "from gensim.utils import deaccent\n",
    "import warnings\n",
    "from inltk.inltk import remove_foreign_languages\n",
    "import regex \n",
    "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
    "from indicnlp.tokenize.indic_tokenize import trivial_tokenize\n",
    "\n",
    "class WhiteSpacePreprocessingStopwords(WhiteSpacePreprocessingStopwords):\n",
    "    \"\"\"\n",
    "    Overriden author's original code to keep accents during preprocessing.\n",
    "    Provides a very simple preprocessing script that filters infrequent tokens from text\n",
    "    \"\"\"\n",
    "    \n",
    "    def tokenize_indic(self, sent, LANG = 'hi'):      \n",
    "      normalizer_factory = IndicNormalizerFactory()\n",
    "      normalizer = normalizer_factory.get_normalizer(LANG)\n",
    "      normalized = normalizer.normalize(sent)\n",
    "      tokenized = ' '.join(trivial_tokenize(normalized, LANG))\n",
    "      return tokenized\n",
    "    \n",
    "    def custom_analyzer(self, text):\n",
    "        \"\"\"\n",
    "        code source: https://stackoverflow.com/questions/60763030\n",
    "                      having-an-issue-in-doing-count-vectorization-for-hindi-text\n",
    "        \"\"\"\n",
    "        words = regex.findall(r'\\w{2,}', text) # extract words of at least 2 letters\n",
    "        for w in words:\n",
    "            yield w\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"\n",
    "        Note that if after filtering some documents do not contain words we remove them. That is why we return also the\n",
    "        list of unpreprocessed documents.\n",
    "        :return: preprocessed documents, unpreprocessed documents and the vocabulary list\n",
    "        \"\"\"\n",
    "        preprocessed_docs_tmp = self.documents\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        if self.remove_numbers:\n",
    "            preprocessed_docs_tmp = [doc.translate(str.maketrans(\"0123456789\", ' ' * len(\"0123456789\")))\n",
    "                                     for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "     \n",
    "        preprocessed_docs_tmp = [self.tokenize_indic(sent) for sent in preprocessed_docs_tmp]\n",
    "        \n",
    "        # USE CUSTOM ANALYZER TO PRESERVE ACCENTS IN VECTORIZER\n",
    "        vectorizer = CountVectorizer(analyzer = self.custom_analyzer, max_features=self.vocabulary_size, max_df=self.max_df)\n",
    "        vectorizer.fit_transform(preprocessed_docs_tmp)\n",
    "        '''\n",
    "        # 04/24 REPLACE ATTRIBUTE\n",
    "            The get_feature_names attribute was deprecated in scikit-learn 1.0 and removed in scikit-learn 1.1. \n",
    "            To get the feature names, you can use the get_feature_names_out attribute instead.\n",
    "        '''\n",
    "        temp_vocabulary = set(vectorizer.get_feature_names_out()) \n",
    "\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if w in temp_vocabulary])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "\n",
    "        preprocessed_docs, unpreprocessed_docs, retained_indices = [], [], []\n",
    "        for i, doc in enumerate(preprocessed_docs_tmp):\n",
    "            if len(doc) > 0 and len(doc) >= self.min_words:\n",
    "                preprocessed_docs.append(doc)\n",
    "                unpreprocessed_docs.append(self.documents[i])\n",
    "                retained_indices.append(i)\n",
    "\n",
    "        vocabulary = list(set([item for doc in preprocessed_docs for item in doc.split()]))\n",
    "\n",
    "        return preprocessed_docs, unpreprocessed_docs, vocabulary, retained_indices                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Modeling\n",
    "\n",
    "We don't discard the non-preprocessed hindi texts, because we are going to use them as input for obtaining the **contextualized** document representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4003 4003\n"
     ]
    }
   ],
   "source": [
    "### HINDI ###\n",
    "LANG_SELECTED = 'hi'\n",
    "\n",
    "# We select 200 tokens per speech\n",
    "NUM_TOKENS = 200\n",
    "\n",
    "# Import Hindi Stopwords\n",
    "import stopwordsiso as stopwords\n",
    "\n",
    "# Run preprocessing script\n",
    "documents = [line[:NUM_TOKENS].strip() for line in train_speeches[LANG_SELECTED]]\n",
    "sp = WhiteSpacePreprocessingStopwords(documents, stopwords_list = stopwords.stopwords(LANG_SELECTED))\n",
    "preprocessed_documents, unpreprocessed_corpus, vocab, __ = sp.preprocess()\n",
    "\n",
    "# Ensure same length for preprocessed and unpreprocessed\n",
    "print(len(preprocessed_documents), len(unpreprocessed_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kHgQLW0hZwjS",
    "outputId": "9c8d60ca-16a1-478e-ffbd-8cf5adaa9f72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['नागरिकों बनने स्वच्छ भारत बनाने आह्वान प्रधानमंत्री श्री नरेन्द्र मोदी '\n",
      " 'महात्मा गांधी साल पूरे अवसर राष्ट्रीय राजधानी',\n",
      " 'श्री वीं बैठक आज सम्पन्न हुई बैठक प्रधानमंत्री श्री नरेंद्र मोदी श्री आडवाणी '\n",
      " 'श्री शाह श्री भाई पटेल श्री पी',\n",
      " 'प्रधानमंत्री श्री नरेन्द्र मोदी अध्यक्षता मंत्रीमंडल बैठक दीनदयाल उपाध्याय '\n",
      " 'ग्राम योजना प्रारंभ आज अनुमति दी गई योजना तहत ग्रामीण क्षेत्रों',\n",
      " 'प्रधानमंत्री श्री नरेन्द्र मोदी आज दिन मुंबई आतंकी हमलों दौरान शहीद सलाम '\n",
      " 'जिन्होंने जिंदगी दे',\n",
      " 'जिला सौर ऊर्जा संयंत्र उद्घाटन किया प्रधानमंत्री मुद्रा योजना विशाल ऋण '\n",
      " 'उद्घाटन लाभार्थियों ऋण दस्तावेज मुद्रा सौ']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(preprocessed_documents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnIptVxk0F4n"
   },
   "source": [
    "**TopicModelDataPreparation**\n",
    "\n",
    "We will now pass our files with preprocess and unpreprocessed data to our TopicModelDataPreparation object. This object takes care of creating the bag of words and obtains the contextualized BERT representations of documents. This operation allows us to create our training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify TopicModelDataPrepation to preserve accents in vectorizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "es7y5swOOtDZ"
   },
   "outputs": [],
   "source": [
    "from contextualized_topic_models.utils import data_preparation as dp\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.sparse\n",
    "import warnings\n",
    "from contextualized_topic_models.datasets.dataset import CTMDataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class TopicModelDataPreparation(TopicModelDataPreparation):\n",
    "      def custom_analyzer(self, text):\n",
    "        \"\"\"\n",
    "        code source: https://stackoverflow.com/questions/60763030\n",
    "                  having-an-issue-in-doing-count-vectorization-for-hindi-text\n",
    "        \"\"\"\n",
    "        words = regex.findall(r'\\w{2,}', text) # extract words of at least 2 letters\n",
    "        for w in words:\n",
    "          yield w\n",
    "\n",
    "      def fit(self, text_for_contextual, text_for_bow, labels=None, custom_embeddings=None):\n",
    "        \"\"\"\n",
    "        This method fits the vectorizer and gets the embeddings from the contextual model\n",
    "        :param text_for_contextual: list of unpreprocessed documents to generate the contextualized embeddings\n",
    "        :param text_for_bow: list of preprocessed documents for creating the bag-of-words\n",
    "        :param custom_embeddings: np.ndarray type object to use custom embeddings (optional).\n",
    "        :param labels: list of labels associated with each document (optional).\n",
    "        \"\"\"\n",
    "\n",
    "        if custom_embeddings is not None:\n",
    "            assert len(text_for_contextual) == len(custom_embeddings)\n",
    "\n",
    "            if text_for_bow is not None:\n",
    "                assert len(custom_embeddings) == len(text_for_bow)\n",
    "\n",
    "            if type(custom_embeddings).__module__ != 'numpy':\n",
    "                raise TypeError(\"contextualized_embeddings must be a numpy.ndarray type object\")\n",
    "\n",
    "        if text_for_bow is not None:\n",
    "            assert len(text_for_contextual) == len(text_for_bow)\n",
    "\n",
    "        if self.contextualized_model is None and custom_embeddings is None:\n",
    "            raise Exception(\"A contextualized model or contextualized embeddings must be defined\")\n",
    "\n",
    "        # TODO: this count vectorizer removes tokens that have len = 1, might be unexpected for the users\n",
    "        self.vectorizer = CountVectorizer(analyzer = self.custom_analyzer)\n",
    "\n",
    "        train_bow_embeddings = self.vectorizer.fit_transform(text_for_bow)\n",
    "\n",
    "        # if the user is passing custom embeddings we don't need to create the embeddings using the model\n",
    "        if custom_embeddings is None:\n",
    "            train_contextualized_embeddings = dp.bert_embeddings_from_list(\n",
    "                text_for_contextual, sbert_model_to_load=self.contextualized_model, max_seq_length=self.max_seq_length)\n",
    "        else:\n",
    "            train_contextualized_embeddings = custom_embeddings\n",
    "        self.vocab = self.vectorizer.get_feature_names_out()\n",
    "        self.id2token = {k: v for k, v in zip(range(0, len(self.vocab)), self.vocab)}\n",
    "\n",
    "        if labels:\n",
    "            self.label_encoder = OneHotEncoder()\n",
    "            encoded_labels = self.label_encoder.fit_transform(np.array([labels]).reshape(-1, 1))\n",
    "        else:\n",
    "            encoded_labels = None\n",
    "        return CTMDataset(\n",
    "            X_contextual=train_contextualized_embeddings, X_bow=train_bow_embeddings,\n",
    "            idx2token=self.id2token, labels=encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# Directory to store models\n",
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/models'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELS_PATH = os.getcwd() + \"/models\"\n",
    "MODELS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here we use the contextualized model \"ai4bharat/indic-bert\", because we need a **multilingual model for indic languages** for performing cross-lingual predictions later.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "be04f806f2e345f9ae96dc97843a2fc8",
      "583ed74f21e647f293b8a93bc7c464d7",
      "fa93dc333cee43cbb80ba1c5b31e8d7c",
      "8950ed9af59f4c64af9f6664eb7cc59d",
      "91f10894dc21445596446cf4f2d013b1",
      "7eca32698dff4e3c980ea6fd3e54d638",
      "82fff8dca4ee4c4fa691531c253a1d0d",
      "9775725241b14076a6321a5403d0cdb2",
      "7e026be532df488a9b40b4278175d2ee",
      "89ce401fdd60417c90fc8191c6d0100a",
      "4cc61eb3f8d54bac840aed2fc768c231"
     ]
    },
    "id": "GO-88XsA0G6m",
    "outputId": "9fdd2261-43e3-46ec-e668-7b5a910c91c2"
   },
   "outputs": [],
   "source": [
    "# Load Indic Multilingual embeddings \n",
    "tp = TopicModelDataPreparation('ai4bharat/indic-bert')\n",
    "tp.max_seq_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8758e6427b6e48b6a58f9e663f9cfa82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)2f401/.gitattributes:   0%|          | 0.00/345 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50a435616134434a9415ddb44145b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)a9c632f401/README.md:   0%|          | 0.00/4.83k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69438874a49c44e798e75ddec8bbdc6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)c632f401/config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca1600d961d484097469dd72e956aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a303125816e4f93a3f0010f466b1751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)632f401/spiece.model:   0%|          | 0.00/5.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439516e812c546f6b883b65d984d5edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)632f401/spiece.vocab:   0%|          | 0.00/5.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c6725c8da94182b5aee5ed984ba122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…).data-00000-of-00001:   0%|          | 0.00/400M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13ffc95e7f042929fac86a8cef4b867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/tf_model.ckpt.index:   0%|          | 0.00/1.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f6092cc4ab4e18800ba5549d654f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)1/tf_model.ckpt.meta:   0%|          | 0.00/2.20M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/ai4bharat_indic-bert. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/ai4bharat_indic-bert were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'sop_classifier.classifier.weight', 'predictions.LayerNorm.bias', 'sop_classifier.classifier.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0e19db58124fc086b16f2c16117268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Building training dataset\n",
    "training_dataset = tp.fit(text_for_contextual=unpreprocessed_corpus, text_for_bow=preprocessed_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 25 and 50 topics models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8fmUu7dz_btU",
    "outputId": "73ec44bd-f114-45e8-cd90-dae1d208c924"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]\t Seen Samples: [396800/400300]\tTrain Loss: 140.4304196757655\tTime: 0:00:00.574728: : 100it [00:54,  1.82it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 63/63 [00:00<00:00, 157.87it/s]\n",
      "Epoch: [100/100]\t Seen Samples: [396800/400300]\tTrain Loss: 148.09137135167276\tTime: 0:00:00.523492: : 100it [00:55,  1.80it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 63/63 [00:00<00:00, 155.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/workspace/models/z_ctm_50_HI.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train over 100 epochs\n",
    "'''\n",
    "    05/14/23: .save() causing issues, using joblib to save models instead\n",
    "    '''\n",
    "# Save model \n",
    "import joblib \n",
    "\n",
    "### HINDI : 25 TOPICS ###\n",
    "z_ctm_25_HI = ZeroShotTM(bow_size=len(tp.vocab), n_components = 25, contextual_size=768, num_epochs=100)\n",
    "\n",
    "z_ctm_25_HI.fit(training_dataset, n_samples = 30) # run the model\n",
    "#z_ctm_25_HI.save(\"./\") # save the model\n",
    "joblib.dump(z_ctm_25_HI, MODELS_PATH + '/z_ctm_25_HI.pkl')\n",
    "\n",
    "# ### HINDI : 50 TOPICS ###\n",
    "z_ctm_50_HI = ZeroShotTM(bow_size=len(tp.vocab), n_components = 50, contextual_size=768, num_epochs=100)\n",
    "\n",
    "z_ctm_50_HI.fit(training_dataset, n_samples = 30) # run the model\n",
    "#z_ctm_50_HI.save(\"./\") # save the model\n",
    "joblib.dump(z_ctm_50_HI, MODELS_PATH + '/z_ctm_50_HI.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqVQBpN5j66X"
   },
   "source": [
    "### Topic Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWLKRp0HtR4I"
   },
   "source": [
    "**PROBLEM - RESOLVED**\n",
    "Predicted topics do not make sense in the train language - Hindi. Preprocessing works just fine. Something is getting lost in translation during model fit. \n",
    "\n",
    "* PATH 1: Topics are inferences from text. (NLI?)\n",
    "* PATH 2: Topics are just words from within the text. (Extraction)\n",
    "* PATH 3: Topics are lemmetized versions of preprocessed tokens. \n",
    "\n",
    "\n",
    "> *What is topic modeling?*\n",
    "\n",
    "Topic modeling is an unsupervised machine learning technique that’s capable of scanning a set of documents, detecting word and phrase patterns within them, and automatically clustering word groups and similar expressions that best characterize a set of documents.\n",
    "\n",
    "src: Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "aYjDfidEcwOr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['मंजूरी', 'दी', 'दे', 'अध्यक्षता', 'केन्द्रीय'],\n",
       " ['हैं', 'संदेश', 'शुभकामनाएं', 'लोगों', 'अवसर'],\n",
       " ['की', 'मुलाकात', 'आज', 'श्री', 'बातचीत'],\n",
       " ['बहनों', 'संख्या', 'विशाल', 'प्यारे', 'भाइयों']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See topic predictions per speech doc\n",
    "z_ctm_25_HI.get_topic_lists(5)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "d-ejLxGsnw5C",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Excellency', 'President', 'of', 'the', 'Your'],\n",
       " ['की', 'मुलाकात', 'आज', 'श्री', 'बातचीत'],\n",
       " ['श्री', 'प्रधानमंत्री', 'मोदी', 'निधन', 'बधाई'],\n",
       " ['बजट', 'देश', 'सत्र', 'संसद', 'होगा']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See topic predictions per speech doc\n",
    "z_ctm_50_HI.get_topic_lists(5)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "FEsfeGMl3d2I"
   },
   "outputs": [],
   "source": [
    "# Save Output in .csv files\n",
    "'''\n",
    "The string contains non-ASCII characters, since text is in Hindi.\n",
    "Open file in a mode that supports Unicode by specifying the encoding when you open the file. \n",
    "'''\n",
    "hi_topics_25 = open(\"hi_topics_25.csv\",\"w\", encoding='utf-8')\n",
    "hi_topics_50 = open(\"hi_topics_50.csv\",\"w\", encoding='utf-8')\n",
    "\n",
    "import csv\n",
    "wr1 = csv.writer(hi_topics_25)\n",
    "wr1.writerows(z_ctm_25_HI.get_topic_lists(5))\n",
    "\n",
    "wr2 = csv.writer(hi_topics_50)\n",
    "wr2.writerows(z_ctm_50_HI.get_topic_lists(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NPMI Coherence for Hindi-Based ZeroShotTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "uwYx_kW3oLun"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPMI Coherences\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t(25)</th>\n",
       "      <th>t(50)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ZeroShotTM for Hindi</th>\n",
       "      <td>0.168529</td>\n",
       "      <td>0.171727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         t(25)     t(50)\n",
       "ZeroShotTM for Hindi  0.168529  0.171727"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get NPMI Coherence\n",
    "from contextualized_topic_models.evaluation.measures import CoherenceNPMI\n",
    "texts = [doc.split() for doc in preprocessed_documents] # load text for NPMI\n",
    "\n",
    "### 25 TOPICS ###\n",
    "npmi_HI = CoherenceNPMI(texts=texts, topics=z_ctm_25_HI.get_topic_lists(25))\n",
    "#print(npmi_HI.score())\n",
    "\n",
    "### 50 TOPICS ###\n",
    "npmi_50_HI = CoherenceNPMI(texts=texts, topics=z_ctm_50_HI.get_topic_lists(50))\n",
    "#print(npmi_50_HI.score())\n",
    "\n",
    "# Store NPMI scores\n",
    "zeroshotNPMI_HI = [npmi_HI.score(), npmi_50_HI.score()]\n",
    "\n",
    "# SHOW RESULTS\n",
    "NPMI_HI = {\"ZeroShotTM for Hindi\" : zeroshotNPMI_HI,}\n",
    "\n",
    "# Set new axis labels and print the DataFrame\n",
    "npmi_HI = pd.DataFrame.from_dict(NPMI_HI, orient='index')\n",
    "\n",
    "print(\"NPMI Coherences\")\n",
    "npmi_HI.set_axis([\"t(25)\", \"t(50)\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJOkb0WAi617"
   },
   "source": [
    "## **English-Based ZeroshotTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H904cm64wssn"
   },
   "source": [
    "### Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "0gQ4SObQLMin"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4107 4107\n"
     ]
    }
   ],
   "source": [
    "### ENGLISH ###\n",
    "LANG_SELECTED = 'en'\n",
    "\n",
    "# We select 200 tokens per speech\n",
    "NUM_TOKENS = 200\n",
    "\n",
    "# Download English Stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Run preprocessing script\n",
    "documents = [line[:NUM_TOKENS].strip() for line in train_speeches[LANG_SELECTED]]\n",
    "\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "# preprocessed_documents\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "documents = [\"\".join([word for word in doc if word.lower() in words or not word.isalpha()]) for doc in documents]\n",
    "\n",
    "sp = WhiteSpacePreprocessing(documents, stopwords_language='english')\n",
    "preprocessed_documents, unpreprocessed_corpus, vocab, __ = sp.preprocess()\n",
    "\n",
    "# Ensure same length for preprocessed and unpreprocessed\n",
    "print(len(preprocessed_documents), len(unpreprocessed_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pm calls upon citizens become create swachh bharat prime minister shri '\n",
      " 'narendra modi inaugurate exhibition titled ek abhiyan',\n",
      " 'first 10 months prime minister narendra term marked high international '\n",
      " 'visits india us president barack obama xi',\n",
      " 'brics africa growth shared prosperity 4th industrial convention centre south '\n",
      " '27 july 2018 10th brics summit',\n",
      " 'meeting shri trust held today meeting attended prime minister shri narendra '\n",
      " 'modi shri lal krishna advani shri sh',\n",
      " 'deendayal upadhyaya yojana union cabinet chaired prime minister shri '\n",
      " 'narendra modi today approved launch deendayal upadhyaya yojana']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(preprocessed_documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Uj-7NMBgN2oW"
   },
   "outputs": [],
   "source": [
    "# Load English embeddings\n",
    "tp = TopicModelDataPreparation(\"sentence-transformers/bert-base-nli-mean-tokens\")\n",
    "tp.max_seq_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a1ae8a6d254cadba371b2329d6eac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)821d1/.gitattributes:   0%|          | 0.00/391 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a86434ce3df459e8d100f5e266d8b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a11f3564364c1d8237ec2c7ebcda3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)8d01e821d1/README.md:   0%|          | 0.00/3.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c17830bdd1d40ed82cc232e37fdb910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)d1/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022d7c36eb3e44ecbc511307826a5c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)01e821d1/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9a8915bb3749388290f9a01ce42774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d920ad964f24a5587aee40bb050cad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89557e26bd3e4dc9b221a91687ef80bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cea9daca8bf4f13b6849e93bdadc919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34ee74efd7049a0910d4419eb02e3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)821d1/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895d29ccf3db459d8e20bb4fdb8be0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33bb628f236a466a95be870ab51935bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)8d01e821d1/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee815359bbcb407fba80dc96d1008d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)1e821d1/modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n",
      "/usr/local/lib/python3.8/dist-packages/ipykernel/comm/comm.py:79: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d488de5487754d09aaf83fe91fd8fe0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Building training dataset\n",
    "en_training = tp.fit(text_for_contextual=unpreprocessed_corpus, text_for_bow=preprocessed_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLv6u6EklqNw"
   },
   "source": [
    "### Training 25 and 50 topics models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "uYP1dTKrOFGy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]\t Seen Samples: [409600/410700]\tTrain Loss: 119.7231285572052\tTime: 0:00:00.595500: : 100it [01:04,  1.55it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65/65 [00:00<00:00, 128.21it/s]\n",
      "Epoch: [100/100]\t Seen Samples: [409600/410700]\tTrain Loss: 127.82322633266449\tTime: 0:00:00.641278: : 100it [01:05,  1.53it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65/65 [00:00<00:00, 128.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/workspace/models/z_ctm_50_EN.pkl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train over 100 epochs\n",
    "\n",
    "### ENGLISH : 25 TOPICS ###\n",
    "z_ctm_25_EN = ZeroShotTM(bow_size=len(tp.vocab), n_components = 25, contextual_size=768, num_epochs=100)\n",
    "z_ctm_25_EN.fit(en_training, n_samples=30) # run the model\n",
    "\n",
    "#z_ctm_25_EN.save(\"./\") # save the model\n",
    "joblib.dump(z_ctm_25_EN, MODELS_PATH + '/z_ctm_25_EN.pkl')\n",
    "\n",
    "### ENGLISH : 50 TOPICS ###\n",
    "z_ctm_50_EN = ZeroShotTM(bow_size=len(tp.vocab), n_components = 50, contextual_size=768, num_epochs=100)\n",
    "z_ctm_50_EN.fit(en_training, n_samples=30) # run the model\n",
    "\n",
    "#z_ctm_50_EN.save(\"./\") # save the model\n",
    "joblib.dump(z_ctm_50_EN, MODELS_PATH + '/z_ctm_50_EN.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMHa8mAZjj3p"
   },
   "source": [
    "### Topic Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "mgZAySpvOy4N"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['people', 'day', 'greeted', 'greetings', 'occasion'],\n",
       " ['approval', 'given', 'chaired', 'union', 'cabinet'],\n",
       " ['today', 'development', 'meeting', 'projects', 'visited'],\n",
       " ['chaired', 'union', 'cabinet', 'mou', 'memorandum']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See topic predictions per speech doc\n",
    "z_ctm_25_EN.get_topic_lists(5)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "u1aSe9afj3pN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cabinet', 'union', 'chaired', 'understanding', 'memorandum'],\n",
       " ['tomorrow', 'inaugurate', 'new', 'arrive', 'delhi'],\n",
       " ['minister', 'prime', 'nepal', 'shri', 'nawaz'],\n",
       " ['condoled', 'passing', 'demise', 'away', 'shri']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See topic predictions per speech doc\n",
    "z_ctm_50_EN.get_topic_lists(5)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Output in .csv files\n",
    "'''\n",
    "The string contains non-ASCII characters, since text is in Hindi.\n",
    "Open file in a mode that supports Unicode by specifying the encoding when you open the file. \n",
    "'''\n",
    "en_topics_25 = open(\"en_topics_25.csv\",\"w\", encoding='utf-8')\n",
    "en_topics_50 = open(\"en_topics_50.csv\",\"w\", encoding='utf-8')\n",
    "\n",
    "import csv\n",
    "wr1 = csv.writer(en_topics_25)\n",
    "wr1.writerows(z_ctm_25_EN.get_topic_lists(5))\n",
    "\n",
    "wr2 = csv.writer(en_topics_50)\n",
    "wr2.writerows(z_ctm_50_EN.get_topic_lists(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NPMI Coherence for English-Based ZeroShotTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "XQrm1TQXogzy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPMI Coherences\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t(25)</th>\n",
       "      <th>t(50)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ZeroShotTM for English</th>\n",
       "      <td>0.105569</td>\n",
       "      <td>0.146268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           t(25)     t(50)\n",
       "ZeroShotTM for English  0.105569  0.146268"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get NPMI Coherence\n",
    "from contextualized_topic_models.evaluation.measures import CoherenceNPMI\n",
    "texts = [doc.split() for doc in preprocessed_documents] # load text for NPMI\n",
    "\n",
    "### 25 TOPICS ###-\n",
    "npmi_EN = CoherenceNPMI(texts=texts, topics=z_ctm_25_EN.get_topic_lists(25))\n",
    "# print(npmi_EN.score())\n",
    "\n",
    "### 50 TOPICS ###\n",
    "npmi_50_EN = CoherenceNPMI(texts=texts, topics=z_ctm_50_EN.get_topic_lists(50))\n",
    "# print(npmi_50_EN.score())\n",
    "\n",
    "# Store NPMI scores\n",
    "zeroshotNPMI_EN = [npmi_EN.score(), npmi_50_EN.score()]\n",
    "\n",
    "# SHOW RESULTS\n",
    "NPMI_EN = {\"ZeroShotTM for English\" : zeroshotNPMI_EN}\n",
    "\n",
    "npmi_EN = pd.DataFrame.from_dict(NPMI_EN, orient='index')\n",
    "print(\"NPMI Coherences\")\n",
    "npmi_EN.set_axis([\"t(25)\", \"t(50)\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9PnZD8oouRq"
   },
   "source": [
    "# Coherence Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "-wLgjEMMowtT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPMI Coherences\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t(25)</th>\n",
       "      <th>t(50)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ZeroShotTM for Hindi</th>\n",
       "      <td>0.168529</td>\n",
       "      <td>0.171727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZeroShotTM for English</th>\n",
       "      <td>0.105569</td>\n",
       "      <td>0.146268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           t(25)     t(50)\n",
       "ZeroShotTM for Hindi    0.168529  0.171727\n",
       "ZeroShotTM for English  0.105569  0.146268"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SHOW RESULTS\n",
    "NPMI = {\"ZeroShotTM for Hindi\" : zeroshotNPMI_HI,\n",
    "        \"ZeroShotTM for English\" : zeroshotNPMI_EN}\n",
    "\n",
    "npmi = pd.DataFrame.from_dict(NPMI, orient='index')\n",
    "print(\"NPMI Coherences\")\n",
    "npmi.set_axis([\"t(25)\", \"t(50)\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ID4xeEywiyE1"
   },
   "source": [
    "# Predictions and Evaluation\n",
    "## **Unseen Multilingual  Corpora Predictions**\n",
    "\n",
    "*Languages*\n",
    "\n",
    "* Assamese - as\n",
    "* Bengali - bn\n",
    "* English - en\n",
    "* Gujarati - gu\n",
    "* Hindi - hi\n",
    "* Kannada - kn\n",
    "* Malayalam - ml\n",
    "* Marathi - mr\n",
    "* Oriya - or\n",
    "* Punjabi - pa\n",
    "* Tamil - ta\n",
    "* Telugu - te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Testsets for Indic Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "with open('parallel_speeches.pkl', 'rb') as test_s:\n",
    "    parallel_speeches = pickle.load(test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "IcyhlPNu3CiQ"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parallel_speeches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert test files into test datasets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m as_testset \u001b[38;5;241m=\u001b[39m tp\u001b[38;5;241m.\u001b[39mtransform(\u001b[43mparallel_speeches\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mas\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m bn_testset \u001b[38;5;241m=\u001b[39m tp\u001b[38;5;241m.\u001b[39mtransform(parallel_speeches[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbn\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m en_testset \u001b[38;5;241m=\u001b[39m tp\u001b[38;5;241m.\u001b[39mtransform(parallel_speeches[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parallel_speeches' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert test files into test datasets\n",
    "as_testset = tp.transform(parallel_speeches['as'])\n",
    "bn_testset = tp.transform(parallel_speeches['bn'])\n",
    "en_testset = tp.transform(parallel_speeches['en'])\n",
    "gu_testset = tp.transform(parallel_speeches['gu'])\n",
    "hi_testset = tp.transform(parallel_speeches['hi'])\n",
    "kn_testset = tp.transform(parallel_speeches['kn'])\n",
    "ml_testset = tp.transform(parallel_speeches['ml'])\n",
    "mr_testset = tp.transform(parallel_speeches['mr'])\n",
    "or_testset = tp.transform(parallel_speeches['or'])\n",
    "pa_testset = tp.transform(parallel_speeches['pa'])\n",
    "ta_testset = tp.transform(parallel_speeches['ta'])\n",
    "te_testset = tp.transform(parallel_speeches['te'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vq0rRIiuo6Qx"
   },
   "source": [
    "### Hindi Topic Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gy-tDYkpcDcT"
   },
   "outputs": [],
   "source": [
    "### HINDI : 25 TOPIC PREDICTIONS ### \n",
    "as_topics_predictions = z_ctm_25_HI.get_thetas(as_testset, n_samples=100) # get all the topic predictions\n",
    "bn_topics_predictions = z_ctm_25_HI.get_thetas(bn_testset, n_samples=100) # get all the topic predictions\n",
    "en_topics_predictions = z_ctm_25_HI.get_thetas(en_testset, n_samples=100) # get all the topic predictions\n",
    "gu_topics_predictions = z_ctm_25_HI.get_thetas(gu_testset, n_samples=100) # get all the topic predictions\n",
    "hi_topics_predictions = z_ctm_25_HI.get_thetas(hi_testset, n_samples=100) # get all the topic predictions\n",
    "kn_topics_predictions = z_ctm_25_HI.get_thetas(kn_testset, n_samples=100) # get all the topic predictions\n",
    "ml_topics_predictions = z_ctm_25_HI.get_thetas(ml_testset, n_samples=100) # get all the topic predictions\n",
    "mr_topics_predictions = z_ctm_25_HI.get_thetas(mr_testset, n_samples=100) # get all the topic predictions\n",
    "or_topics_predictions = z_ctm_25_HI.get_thetas(or_testset, n_samples=100) # get all the topic predictions\n",
    "pa_topics_predictions = z_ctm_25_HI.get_thetas(pa_testset, n_samples=100) # get all the topic predictions\n",
    "ta_topics_predictions = z_ctm_25_HI.get_thetas(ta_testset, n_samples=100) # get all the topic predictions\n",
    "te_topics_predictions = z_ctm_25_HI.get_thetas(te_testset, n_samples=100) # get all the topic predictions\n",
    "\n",
    "topics_25_HI = {'as': as_topics_predictions, 'bn': bn_topics_predictions, \n",
    "             'en': en_topics_predictions, 'gu': gu_topics_predictions,\n",
    "             'hi': hi_topics_predictions, 'kn': kn_topics_predictions,\n",
    "             'ml': ml_topics_predictions, 'mr': mr_topics_predictions,\n",
    "             'or': or_topics_predictions, 'pa': pa_topics_predictions,\n",
    "             'ta': ta_topics_predictions, 'te': te_topics_predictions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5ZFFpd43v6b"
   },
   "outputs": [],
   "source": [
    "### HINDI : 50 TOPIC PREDICTIONS ### \n",
    "as_topics_predictions = z_ctm_50_HI.get_thetas(as_testset, n_samples=100) # get all the topic predictions\n",
    "bn_topics_predictions = z_ctm_50_HI.get_thetas(bn_testset, n_samples=100) # get all the topic predictions\n",
    "en_topics_predictions = z_ctm_50_HI.get_thetas(en_testset, n_samples=100) # get all the topic predictions\n",
    "gu_topics_predictions = z_ctm_50_HI.get_thetas(gu_testset, n_samples=100) # get all the topic predictions\n",
    "hi_topics_predictions = z_ctm_50_HI.get_thetas(hi_testset, n_samples=100) # get all the topic predictions\n",
    "kn_topics_predictions = z_ctm_50_HI.get_thetas(kn_testset, n_samples=100) # get all the topic predictions\n",
    "ml_topics_predictions = z_ctm_50_HI.get_thetas(ml_testset, n_samples=100) # get all the topic predictions\n",
    "mr_topics_predictions = z_ctm_50_HI.get_thetas(mr_testset, n_samples=100) # get all the topic predictions\n",
    "or_topics_predictions = z_ctm_50_HI.get_thetas(or_testset, n_samples=100) # get all the topic predictions\n",
    "pa_topics_predictions = z_ctm_50_HI.get_thetas(pa_testset, n_samples=100) # get all the topic predictions\n",
    "ta_topics_predictions = z_ctm_50_HI.get_thetas(ta_testset, n_samples=100) # get all the topic predictions\n",
    "te_topics_predictions = z_ctm_50_HI.get_thetas(te_testset, n_samples=100) # get all the topic predictions\n",
    "\n",
    "topics_50_HI = {'as': as_topics_predictions, 'bn': bn_topics_predictions, \n",
    "             'en': en_topics_predictions, 'gu': gu_topics_predictions,\n",
    "             'hi': hi_topics_predictions, 'kn': kn_topics_predictions,\n",
    "             'ml': ml_topics_predictions, 'mr': mr_topics_predictions,\n",
    "             'or': or_topics_predictions, 'pa': pa_topics_predictions,\n",
    "             'ta': ta_topics_predictions, 'te': te_topics_predictions}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6Jom4sqpHyo"
   },
   "source": [
    "### English Topic Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Abhs52LpLt3"
   },
   "outputs": [],
   "source": [
    "### ENGLISH : 25 TOPIC PREDICTIONS ### \n",
    "as_topics_predictions = z_ctm_25_EN.get_thetas(as_testset, n_samples=100) # get all the topic predictions\n",
    "bn_topics_predictions = z_ctm_25_EN.get_thetas(bn_testset, n_samples=100) # get all the topic predictions\n",
    "en_topics_predictions = z_ctm_25_EN.get_thetas(en_testset, n_samples=100) # get all the topic predictions\n",
    "gu_topics_predictions = z_ctm_25_EN.get_thetas(gu_testset, n_samples=100) # get all the topic predictions\n",
    "hi_topics_predictions = z_ctm_25_EN.get_thetas(hi_testset, n_samples=100) # get all the topic predictions\n",
    "kn_topics_predictions = z_ctm_25_EN.get_thetas(kn_testset, n_samples=100) # get all the topic predictions\n",
    "ml_topics_predictions = z_ctm_25_EN.get_thetas(ml_testset, n_samples=100) # get all the topic predictions\n",
    "mr_topics_predictions = z_ctm_25_EN.get_thetas(mr_testset, n_samples=100) # get all the topic predictions\n",
    "or_topics_predictions = z_ctm_25_EN.get_thetas(or_testset, n_samples=100) # get all the topic predictions\n",
    "pa_topics_predictions = z_ctm_25_EN.get_thetas(pa_testset, n_samples=100) # get all the topic predictions\n",
    "ta_topics_predictions = z_ctm_25_EN.get_thetas(ta_testset, n_samples=100) # get all the topic predictions\n",
    "te_topics_predictions = z_ctm_25_EN.get_thetas(te_testset, n_samples=100) # get all the topic predictions\n",
    "\n",
    "topics_25_EN = {'as': as_topics_predictions, 'bn': bn_topics_predictions, \n",
    "             'en': en_topics_predictions, 'gu': gu_topics_predictions,\n",
    "             'hi': hi_topics_predictions, 'kn': kn_topics_predictions,\n",
    "             'ml': ml_topics_predictions, 'mr': mr_topics_predictions,\n",
    "             'or': or_topics_predictions, 'pa': pa_topics_predictions,\n",
    "             'ta': ta_topics_predictions, 'te': te_topics_predictions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnJLxNixpL9R"
   },
   "outputs": [],
   "source": [
    "### ENGLISH : 50 TOPIC PREDICTIONS ### \n",
    "as_topics_predictions = z_ctm_50_EN.get_thetas(as_testset, n_samples=100) # get all the topic predictions\n",
    "bn_topics_predictions = z_ctm_50_EN.get_thetas(bn_testset, n_samples=100) # get all the topic predictions\n",
    "en_topics_predictions = z_ctm_50_EN.get_thetas(en_testset, n_samples=100) # get all the topic predictions\n",
    "gu_topics_predictions = z_ctm_50_EN.get_thetas(gu_testset, n_samples=100) # get all the topic predictions\n",
    "hi_topics_predictions = z_ctm_50_EN.get_thetas(hi_testset, n_samples=100) # get all the topic predictions\n",
    "kn_topics_predictions = z_ctm_50_EN.get_thetas(kn_testset, n_samples=100) # get all the topic predictions\n",
    "ml_topics_predictions = z_ctm_50_EN.get_thetas(ml_testset, n_samples=100) # get all the topic predictions\n",
    "mr_topics_predictions = z_ctm_50_EN.get_thetas(mr_testset, n_samples=100) # get all the topic predictions\n",
    "or_topics_predictions = z_ctm_50_EN.get_thetas(or_testset, n_samples=100) # get all the topic predictions\n",
    "pa_topics_predictions = z_ctm_50_EN.get_thetas(pa_testset, n_samples=100) # get all the topic predictions\n",
    "ta_topics_predictions = z_ctm_50_EN.get_thetas(ta_testset, n_samples=100) # get all the topic predictions\n",
    "te_topics_predictions = z_ctm_50_EN.get_thetas(te_testset, n_samples=100) # get all the topic predictions\n",
    "\n",
    "topics_50_EN = {'as': as_topics_predictions, 'bn': bn_topics_predictions, \n",
    "             'en': en_topics_predictions, 'gu': gu_topics_predictions,\n",
    "             'hi': hi_topics_predictions, 'kn': kn_topics_predictions,\n",
    "             'ml': ml_topics_predictions, 'mr': mr_topics_predictions,\n",
    "             'or': or_topics_predictions, 'pa': pa_topics_predictions,\n",
    "             'ta': ta_topics_predictions, 'te': te_topics_predictions}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EyO4qpXm4Nj"
   },
   "source": [
    "# **Quantitative Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUHdC6ztmzr5"
   },
   "outputs": [],
   "source": [
    "# Import metrics\n",
    "from contextualized_topic_models.evaluation.measures import Matches, KLDivergence, CentroidDistance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJvWmhANeWea"
   },
   "source": [
    "1. **Matches**\n",
    "\n",
    "> Matches is the % of times the predicted topic for the non-English test document is the same as for the respective test document in English. The higher the scores, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0BmiGNcrVKD"
   },
   "source": [
    "## Matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aF2GPx1_5Mal"
   },
   "outputs": [],
   "source": [
    "# HINDI : Matches for 25 topics\n",
    "hi_as_matches = Matches(topics_25_HI['hi'], topics_25_HI['as'])\n",
    "hi_bn_matches = Matches(topics_25_HI['hi'], topics_25_HI['bn'])\n",
    "hi_en_matches = Matches(topics_25_HI['hi'], topics_25_HI['en'])\n",
    "hi_gu_matches = Matches(topics_25_HI['hi'], topics_25_HI['gu'])\n",
    "hi_kn_matches = Matches(topics_25_HI['hi'], topics_25_HI['kn'])\n",
    "hi_ml_matches = Matches(topics_25_HI['hi'], topics_25_HI['ml'])\n",
    "hi_mr_matches = Matches(topics_25_HI['hi'], topics_25_HI['mr'])\n",
    "hi_or_matches = Matches(topics_25_HI['hi'], topics_25_HI['or'])\n",
    "hi_pa_matches = Matches(topics_25_HI['hi'], topics_25_HI['pa'])\n",
    "hi_ta_matches = Matches(topics_25_HI['hi'], topics_25_HI['ta'])\n",
    "hi_te_matches = Matches(topics_25_HI['hi'], topics_25_HI['te'])\n",
    "\n",
    "\n",
    "matches_25_HI = {'as': hi_as_matches.score(), 'bn': hi_bn_matches.score(), \n",
    "             'en': hi_en_matches.score(), 'gu': hi_gu_matches.score(),\n",
    "             'kn': hi_kn_matches.score(),\n",
    "             'ml': hi_ml_matches.score(), 'mr': hi_mr_matches.score(),\n",
    "             'or': hi_or_matches.score(), 'pa': hi_pa_matches.score(),\n",
    "             'ta': hi_ta_matches.score(), 'te': hi_te_matches.score()}\n",
    "matches_25_HI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1EIO1fdfWQ0"
   },
   "outputs": [],
   "source": [
    "# Average Matches for 25 topics\n",
    "average_matches_25_HI = sum(matches_25_HI.values())/len(matches_25_HI.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLw8Xkt5dElD"
   },
   "outputs": [],
   "source": [
    "# HINDI : Matches for 50 topics\n",
    "hi_as_matches = Matches(topics_50_HI['hi'], topics_50_HI['as'])\n",
    "hi_bn_matches = Matches(topics_50_HI['hi'], topics_50_HI['bn'])\n",
    "hi_en_matches = Matches(topics_50_HI['hi'], topics_50_HI['en'])\n",
    "hi_gu_matches = Matches(topics_50_HI['hi'], topics_50_HI['gu'])\n",
    "hi_kn_matches = Matches(topics_50_HI['hi'], topics_50_HI['kn'])\n",
    "hi_ml_matches = Matches(topics_50_HI['hi'], topics_50_HI['ml'])\n",
    "hi_mr_matches = Matches(topics_50_HI['hi'], topics_50_HI['mr'])\n",
    "hi_or_matches = Matches(topics_50_HI['hi'], topics_50_HI['or'])\n",
    "hi_pa_matches = Matches(topics_50_HI['hi'], topics_50_HI['pa'])\n",
    "hi_ta_matches = Matches(topics_50_HI['hi'], topics_50_HI['ta'])\n",
    "hi_te_matches = Matches(topics_50_HI['hi'], topics_50_HI['te'])\n",
    "\n",
    "\n",
    "matches_50_HI = {'as': hi_as_matches.score(), 'bn': hi_bn_matches.score(), \n",
    "             'en': hi_en_matches.score(), 'gu': hi_gu_matches.score(),\n",
    "             'kn': hi_kn_matches.score(),\n",
    "             'ml': hi_ml_matches.score(), 'mr': hi_mr_matches.score(),\n",
    "             'or': hi_or_matches.score(), 'pa': hi_pa_matches.score(),\n",
    "             'ta': hi_ta_matches.score(), 'te': hi_te_matches.score()}\n",
    "matches_50_HI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDVjptxefs1N"
   },
   "outputs": [],
   "source": [
    "# Average Matches for 50 topics\n",
    "average_matches_50_HI = sum(matches_50_HI.values())/len(matches_50_HI.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o862P3oerW-8"
   },
   "source": [
    "### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-D9lipprab7"
   },
   "outputs": [],
   "source": [
    "# ENGLISH : Matches for 25 topics\n",
    "en_as_matches = Matches(topics_25_EN['en'], topics_25_EN['as'])\n",
    "en_bn_matches = Matches(topics_25_EN['en'], topics_25_EN['bn'])\n",
    "en_hi_matches = Matches(topics_25_EN['en'], topics_25_EN['hi'])\n",
    "en_gu_matches = Matches(topics_25_EN['en'], topics_25_EN['gu'])\n",
    "en_kn_matches = Matches(topics_25_EN['en'], topics_25_EN['kn'])\n",
    "en_ml_matches = Matches(topics_25_EN['en'], topics_25_EN['ml'])\n",
    "en_mr_matches = Matches(topics_25_EN['en'], topics_25_EN['mr'])\n",
    "en_or_matches = Matches(topics_25_EN['en'], topics_25_EN['or'])\n",
    "en_pa_matches = Matches(topics_25_EN['en'], topics_25_EN['pa'])\n",
    "en_ta_matches = Matches(topics_25_EN['en'], topics_25_EN['ta'])\n",
    "en_te_matches = Matches(topics_25_EN['en'], topics_25_EN['te'])\n",
    "\n",
    "\n",
    "matches_25_EN = {'as': en_as_matches.score(), 'bn': en_bn_matches.score(), \n",
    "             'en': en_hi_matches.score(), 'gu': en_gu_matches.score(),\n",
    "             'kn': en_kn_matches.score(),\n",
    "             'ml': en_ml_matches.score(), 'mr': en_mr_matches.score(),\n",
    "             'or': en_or_matches.score(), 'pa': en_pa_matches.score(),\n",
    "             'ta': en_ta_matches.score(), 'te': en_te_matches.score()}\n",
    "matches_25_EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKSNKSixfxxG"
   },
   "outputs": [],
   "source": [
    "# Average Matches for 25 topics\n",
    "average_matches_25_EN = sum(matches_25_EN.values())/len(matches_25_EN.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3REUgwm-ryeG"
   },
   "outputs": [],
   "source": [
    "# ENGLISH : Matches for 50 topics\n",
    "en_as_matches = Matches(topics_50_EN['en'], topics_50_EN['as'])\n",
    "en_bn_matches = Matches(topics_50_EN['en'], topics_50_EN['bn'])\n",
    "en_en_matches = Matches(topics_50_EN['en'], topics_50_EN['hi'])\n",
    "en_gu_matches = Matches(topics_50_EN['en'], topics_50_EN['gu'])\n",
    "en_kn_matches = Matches(topics_50_EN['en'], topics_50_EN['kn'])\n",
    "en_ml_matches = Matches(topics_50_EN['en'], topics_50_EN['ml'])\n",
    "en_mr_matches = Matches(topics_50_EN['en'], topics_50_EN['mr'])\n",
    "en_or_matches = Matches(topics_50_EN['en'], topics_50_EN['or'])\n",
    "en_pa_matches = Matches(topics_50_EN['en'], topics_50_EN['pa'])\n",
    "en_ta_matches = Matches(topics_50_EN['en'], topics_50_EN['ta'])\n",
    "en_te_matches = Matches(topics_50_EN['en'], topics_50_EN['te'])\n",
    "\n",
    "\n",
    "matches_50_EN = {'as': en_as_matches.score(), 'bn': en_bn_matches.score(), \n",
    "             'en': en_hi_matches.score(), 'gu': en_gu_matches.score(),\n",
    "             'kn': en_kn_matches.score(),\n",
    "             'ml': en_ml_matches.score(), 'mr': en_mr_matches.score(),\n",
    "             'or': en_or_matches.score(), 'pa': en_pa_matches.score(),\n",
    "             'ta': en_ta_matches.score(), 'te': en_te_matches.score()}\n",
    "matches_50_EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDAyFUcGf1k1"
   },
   "outputs": [],
   "source": [
    "# Average Matches for 50 topics\n",
    "average_matches_50_EN = sum(matches_50_EN.values())/len(matches_50_EN.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PysLdKoyZ-fS"
   },
   "source": [
    "2. **Distributional Similarity**\n",
    "> Compute the KL divergence between the predicted topic distribution on the test document and the same test document in English. Lower scores are better, indicating that the distributions do not differ by much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional Similarity (KL Divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qDKyNdesFO_"
   },
   "source": [
    "### Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nc8TYrOCnUt6"
   },
   "outputs": [],
   "source": [
    "# HINDI : KL Divergence for 25 topics\n",
    "hi_as_kl = KLDivergence(topics_25_HI['hi'], topics_25_HI['as'])\n",
    "hi_bn_kl = KLDivergence(topics_25_HI['hi'], topics_25_HI['bn'])\n",
    "hi_en_kl = KLDivergence(topics_25_HI['hi'], topics_25_HI['en'])\n",
    "hi_gu_kl = KLDivergence(topics_25_HI['hi'], topics_25_HI['gu'])\n",
    "hi_kn_kl = KLDivergence(topics_25_HI['hi'], topics_25_HI['kn'])\n",
    "hi_ml_kl = KLDivergence(topics_25_HI['hi'], topics_25_HI['ml'])\n",
    "hi_mr_kl = KLDivergence(topics_25_HI['hi'], topics_25_HI['mr'])\n",
    "hi_or_kl = KLDivergence(topics_25_HI['hi'], topics_25_HI['or'])\n",
    "hi_pa_kl = KLDivergence(topics_25_HI['hi'], topics_25_HI['pa'])\n",
    "hi_ta_kl = KLDivergence(topics_25_HI['hi'], topics_25_HI['ta'])\n",
    "hi_te_kl = KLDivergence(topics_25_HI['hi'], topics_25_HI['te'])\n",
    "\n",
    "kl_divergence_25_HI = {'as': hi_as_kl.score(), 'bn': hi_bn_kl.score(), \n",
    "             'en': hi_en_kl.score(), 'gu': hi_gu_kl.score(),\n",
    "             'kn': hi_kn_kl.score(),\n",
    "             'ml': hi_ml_kl.score(), 'mr': hi_mr_kl.score(),\n",
    "             'or': hi_or_kl.score(), 'pa': hi_pa_kl.score(),\n",
    "             'ta': hi_ta_kl.score(), 'te': hi_te_kl.score()}\n",
    "\n",
    "kl_divergence_25_HI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bo3Jtxunf7wC"
   },
   "outputs": [],
   "source": [
    "# Average KLD for 25 topics\n",
    "average_kl_divergence_25_HI = sum(kl_divergence_25_HI.values())/len(kl_divergence_25_HI.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OztmlUHK5M8s"
   },
   "outputs": [],
   "source": [
    "# HINDI : KL Divergence for 50 topics\n",
    "hi_as_kl = KLDivergence(topics_50_HI['hi'], topics_50_HI['as'])\n",
    "hi_bn_kl = KLDivergence(topics_50_HI['hi'], topics_50_HI['bn'])\n",
    "hi_en_kl = KLDivergence(topics_50_HI['hi'], topics_50_HI['en'])\n",
    "hi_gu_kl = KLDivergence(topics_50_HI['hi'], topics_50_HI['gu'])\n",
    "hi_kn_kl = KLDivergence(topics_50_HI['hi'], topics_50_HI['kn'])\n",
    "hi_ml_kl = KLDivergence(topics_50_HI['hi'], topics_50_HI['ml'])\n",
    "hi_mr_kl = KLDivergence(topics_50_HI['hi'], topics_50_HI['mr'])\n",
    "hi_or_kl = KLDivergence(topics_50_HI['hi'], topics_50_HI['or'])\n",
    "hi_pa_kl = KLDivergence(topics_50_HI['hi'], topics_50_HI['pa'])\n",
    "hi_ta_kl = KLDivergence(topics_50_HI['hi'], topics_50_HI['ta'])\n",
    "hi_te_kl = KLDivergence(topics_50_HI['hi'], topics_50_HI['te'])\n",
    "\n",
    "kl_divergence_50_HI = {'as': hi_as_kl.score(), 'bn': hi_bn_kl.score(), \n",
    "             'en': hi_en_kl.score(), 'gu': hi_gu_kl.score(),\n",
    "             'kn': hi_kn_kl.score(),\n",
    "             'ml': hi_ml_kl.score(), 'mr': hi_mr_kl.score(),\n",
    "             'or': hi_or_kl.score(), 'pa': hi_pa_kl.score(),\n",
    "             'ta': hi_ta_kl.score(), 'te': hi_te_kl.score()}\n",
    "\n",
    "kl_divergence_50_HI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dw7A1oBigDFa"
   },
   "outputs": [],
   "source": [
    "# Average KLD for 50 topics\n",
    "average_kl_divergence_50_HI = sum(kl_divergence_50_HI.values())/len(kl_divergence_50_HI.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyxUuP1bsG_2"
   },
   "source": [
    "### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ieStOUDKsJMO"
   },
   "outputs": [],
   "source": [
    "# ENGLISH : KL Divergence for 25 topics\n",
    "en_as_kl = KLDivergence(topics_25_EN['en'], topics_25_EN['as'])\n",
    "en_bn_kl = KLDivergence(topics_25_EN['en'], topics_25_EN['bn'])\n",
    "en_hi_kl = KLDivergence(topics_25_EN['en'], topics_25_EN['hi'])\n",
    "en_gu_kl = KLDivergence(topics_25_EN['en'], topics_25_EN['gu'])\n",
    "en_kn_kl = KLDivergence(topics_25_EN['en'], topics_25_EN['kn'])\n",
    "en_ml_kl = KLDivergence(topics_25_EN['en'], topics_25_EN['ml'])\n",
    "en_mr_kl = KLDivergence(topics_25_EN['en'], topics_25_EN['mr'])\n",
    "en_or_kl = KLDivergence(topics_25_EN['en'], topics_25_EN['or'])\n",
    "en_pa_kl = KLDivergence(topics_25_EN['en'], topics_25_EN['pa'])\n",
    "en_ta_kl = KLDivergence(topics_25_EN['en'], topics_25_EN['ta'])\n",
    "en_te_kl = KLDivergence(topics_25_EN['en'], topics_25_EN['te'])\n",
    "\n",
    "kl_divergence_25_EN = {'as': en_as_kl.score(), 'bn': en_bn_kl.score(), \n",
    "             'hi': en_hi_kl.score(), 'gu': en_gu_kl.score(),\n",
    "             'kn': en_kn_kl.score(),\n",
    "             'ml': en_ml_kl.score(), 'mr': en_mr_kl.score(),\n",
    "             'or': en_or_kl.score(), 'pa': en_pa_kl.score(),\n",
    "             'ta': en_ta_kl.score(), 'te': en_te_kl.score()}\n",
    "\n",
    "kl_divergence_25_EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRJtyvWmgJYv"
   },
   "outputs": [],
   "source": [
    "# Average KLD for 50 topics\n",
    "average_kl_divergence_25_EN = sum(kl_divergence_25_EN.values())/len(kl_divergence_25_EN.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ierbd5Uws0T1"
   },
   "outputs": [],
   "source": [
    "# ENGLISH : KL Divergence for 50 topics\n",
    "en_as_kl = KLDivergence(topics_50_EN['en'], topics_50_EN['as'])\n",
    "en_bn_kl = KLDivergence(topics_50_EN['en'], topics_50_EN['bn'])\n",
    "en_hi_kl = KLDivergence(topics_50_EN['en'], topics_50_EN['hi'])\n",
    "en_gu_kl = KLDivergence(topics_50_EN['en'], topics_50_EN['gu'])\n",
    "en_kn_kl = KLDivergence(topics_50_EN['en'], topics_50_EN['kn'])\n",
    "en_ml_kl = KLDivergence(topics_50_EN['en'], topics_50_EN['ml'])\n",
    "en_mr_kl = KLDivergence(topics_50_EN['en'], topics_50_EN['mr'])\n",
    "en_or_kl = KLDivergence(topics_50_EN['en'], topics_50_EN['or'])\n",
    "en_pa_kl = KLDivergence(topics_50_EN['en'], topics_50_EN['pa'])\n",
    "en_ta_kl = KLDivergence(topics_50_EN['en'], topics_50_EN['ta'])\n",
    "en_te_kl = KLDivergence(topics_50_EN['en'], topics_50_EN['te'])\n",
    "\n",
    "kl_divergence_50_EN = {'as': en_as_kl.score(), 'bn': en_bn_kl.score(), \n",
    "             'hi': en_hi_kl.score(), 'gu': en_gu_kl.score(),\n",
    "             'kn': en_kn_kl.score(),\n",
    "             'ml': en_ml_kl.score(), 'mr': en_mr_kl.score(),\n",
    "             'or': en_or_kl.score(), 'pa': en_pa_kl.score(),\n",
    "             'ta': en_ta_kl.score(), 'te': en_te_kl.score()}\n",
    "\n",
    "kl_divergence_50_EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LCOLSftgPGR"
   },
   "outputs": [],
   "source": [
    "# Average KLD for 50 topics\n",
    "average_kl_divergence_50_EN = sum(kl_divergence_50_EN.values())/len(kl_divergence_50_EN.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzpXFJ0wZ5-2"
   },
   "source": [
    "3. **Centroid Embeddings**\n",
    "> To also account for similar but not exactly equal topic predictions, we compute the centroid embeddings of the 5 words describing the predicted topic for both English and non-English documents. Then we compute the cosine similarity between those two centroids (CD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centroid Embeddings Distance (CD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbXgKs-u5NyD"
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "from scipy.spatial.distance import cosine\n",
    "import abc\n",
    "import numpy as np\n",
    "\n",
    "class CD(CentroidDistance):\n",
    "    \"\"\"Override author's function to upgrade compatibility with Gensim 4.0.0.\n",
    "    See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4.\"\"\"\n",
    "\n",
    "    def get_centroid(self, word_list):\n",
    "        vector_list = []\n",
    "        for word in word_list:\n",
    "            if word in self.wv:   # changed from self.wv.vocab to self.wv as in Gensim 4.0.0\n",
    "                vector_list.append(self.wv.get_vector(word))\n",
    "        vec = sum(vector_list)\n",
    "        return vec / np.linalg.norm(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Hp8OuALtmOF"
   },
   "source": [
    "### Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CjnYIIng5MyA"
   },
   "outputs": [],
   "source": [
    "# HINDI : Centroid Embeddings for 25 topics\n",
    "cd_25_HI = {}\n",
    "\n",
    "for key in topics_25_HI.keys():\n",
    "  if key == 'hi':\n",
    "    continue\n",
    "  topic = topics_25_HI[key]\n",
    "  cd = CD(doc_distribution_original_language = topics_25_HI['hi'], \n",
    "          doc_distribution_unseen_language = topic, \n",
    "          topics = z_ctm_25_HI.get_topic_lists(25),\n",
    "          topk = 5)\n",
    "  \n",
    "  cd_25_HI[key] = cd.score()\n",
    "\n",
    "cd_25_HI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-w6fflJFgmsm"
   },
   "outputs": [],
   "source": [
    "# Average KLD for 25 topics\n",
    "average_cd_25_HI = sum(cd_25_HI.values())/len(cd_25_HI.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXRBx6VjW-5q"
   },
   "outputs": [],
   "source": [
    "# HINDI : Centroid Embeddings for 50 topics\n",
    "cd_50_HI = {}\n",
    "\n",
    "for key in topics_50_HI.keys():\n",
    "  if key == 'hi':\n",
    "    continue\n",
    "  topic = topics_50_HI[key]\n",
    "  cd = CD(doc_distribution_original_language = topics_50_HI['hi'], \n",
    "          doc_distribution_unseen_language = topic, \n",
    "          topics = z_ctm_50_HI.get_topic_lists(50),\n",
    "          topk = 5)\n",
    "  \n",
    "  cd_50_HI[key] = cd.score()\n",
    "  cd = None\n",
    "\n",
    "cd_50_HI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bil_aX4TtoHu"
   },
   "source": [
    "### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orOyp9W4gszO"
   },
   "outputs": [],
   "source": [
    "# Average KLD for 25 topics\n",
    "average_cd_25_HI = sum(cd_25_HI.values())/len(cd_25_HI.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svDMq5Egtrb5"
   },
   "outputs": [],
   "source": [
    "# ENGLISH : Centroid Embeddings for 25 topics\n",
    "cd_25_EN = {}\n",
    "\n",
    "for key in topics_25_EN.keys():\n",
    "  if key == 'en':\n",
    "    continue\n",
    "  topic = topics_25_EN[key]\n",
    "  cd = CD(doc_distribution_original_language = topics_25_EN['en'], \n",
    "          doc_distribution_unseen_language = topic, \n",
    "          topics = z_ctm_25_EN.get_topic_lists(25),\n",
    "          topk = 5)\n",
    "  \n",
    "  cd_25_EN[key] = cd.score()\n",
    "\n",
    "cd_25_EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9knOxB92tsmZ"
   },
   "outputs": [],
   "source": [
    "# ENGLISH : Centroid Embeddings for 50 topics\n",
    "cd_50_EN = {}\n",
    "\n",
    "for key in topics_50_EN.keys():\n",
    "  if key == 'hi':\n",
    "    continue\n",
    "  topic = topics_50_EN[key]\n",
    "  cd = CD(doc_distribution_original_language = topics_50_EN['en'], \n",
    "          doc_distribution_unseen_language = topic, \n",
    "          topics = z_ctm_50_EN.get_topic_lists(50),\n",
    "          topk = 5)\n",
    "  \n",
    "  cd_50_EN[key] = cd.score()\n",
    "  cd = None\n",
    "\n",
    "cd_50_EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obeCKsgJ9gf4"
   },
   "outputs": [],
   "source": [
    "# Store Metrics\n",
    "metrics = {\n",
    "          \"Hindi\" : [{\n",
    "                    \"Mat25\": matches_25_HI,\n",
    "                    \"KL25\": kl_divergence_25_HI, \n",
    "                    \"CD25\": cd_25_HI, \n",
    "                    \"Mat50\": matches_50_HI, \n",
    "                    \"KL50\": kl_divergence_50_HI,\n",
    "                    \"CD50\": cd_50_HI\n",
    "                    }],\n",
    "\n",
    "          \"English\": [{\n",
    "                    \"Mat25\": matches_25_EN,\n",
    "                    \"KL25\": kl_divergence_25_EN, \n",
    "                    \"CD25\": cd_25_EN, \n",
    "                    \"Mat50\": matches_50_EN, \n",
    "                    \"KL50\": kl_divergence_50_EN,\n",
    "                    \"CD50\": cd_50_EN\n",
    "                    }]\n",
    "          }\n",
    "with open(\"metrics_samescript.txt\", 'wb') as F:\n",
    "  pickle.dump(metrics, F)\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzAMmhgGpTb5"
   },
   "source": [
    "# Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wOmqj7DsXs_"
   },
   "outputs": [],
   "source": [
    "# Show results\n",
    "metrics = {\"Hindi\" : \n",
    "           \n",
    "           {\"Mat25\": matches_25_HI,\n",
    "           \"KL25\": kl_divergence_25_HI, \n",
    "           \"CD25\": cd_25_HI, \n",
    "           \"Mat50\": matches_50_HI, \n",
    "           \"KL50\": kl_divergence_50_HI,\n",
    "           \"CD50\": cd_50_HI},\n",
    "           \n",
    "           \"English\" : \n",
    "           {\"Mat25\": matches_25_EN,\n",
    "           \"KL25\": kl_divergence_25_EN, \n",
    "           \"CD25\": cd_25_EN, \n",
    "           \"Mat50\": matches_50_EN, \n",
    "           \"KL50\": kl_divergence_50_EN,\n",
    "           \"CD50\": cd_50_EN}\n",
    "           }\n",
    "\n",
    "metrics = pd.DataFrame.from_dict(metrics, orient='columns') \n",
    "print(\"Match, KL, and Centroid Similarity for 25 and 50 topics on various languages on PMIndia Corpus\")\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [
    "YMHa8mAZjj3p",
    "i9PnZD8oouRq"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "4cc61eb3f8d54bac840aed2fc768c231": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "583ed74f21e647f293b8a93bc7c464d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7eca32698dff4e3c980ea6fd3e54d638",
      "placeholder": "​",
      "style": "IPY_MODEL_82fff8dca4ee4c4fa691531c253a1d0d",
      "value": "Batches: 100%"
     }
    },
    "7e026be532df488a9b40b4278175d2ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7eca32698dff4e3c980ea6fd3e54d638": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82fff8dca4ee4c4fa691531c253a1d0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8950ed9af59f4c64af9f6664eb7cc59d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89ce401fdd60417c90fc8191c6d0100a",
      "placeholder": "​",
      "style": "IPY_MODEL_4cc61eb3f8d54bac840aed2fc768c231",
      "value": " 21/21 [00:04&lt;00:00,  5.21it/s]"
     }
    },
    "89ce401fdd60417c90fc8191c6d0100a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91f10894dc21445596446cf4f2d013b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9775725241b14076a6321a5403d0cdb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be04f806f2e345f9ae96dc97843a2fc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_583ed74f21e647f293b8a93bc7c464d7",
       "IPY_MODEL_fa93dc333cee43cbb80ba1c5b31e8d7c",
       "IPY_MODEL_8950ed9af59f4c64af9f6664eb7cc59d"
      ],
      "layout": "IPY_MODEL_91f10894dc21445596446cf4f2d013b1"
     }
    },
    "fa93dc333cee43cbb80ba1c5b31e8d7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9775725241b14076a6321a5403d0cdb2",
      "max": 21,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7e026be532df488a9b40b4278175d2ee",
      "value": 21
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
